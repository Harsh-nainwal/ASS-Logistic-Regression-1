{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d775125-9764-4772-8546-f248ead78c72",
   "metadata": {},
   "source": [
    "Q1. Difference between Linear Regression and Logistic Regression:\n",
    "\n",
    "Ans:-\n",
    "\n",
    "Linear Regression: Used for predicting continuous numerical outcomes. It establishes a linear relationship between input features and the target variable.\n",
    "Logistic Regression: Used for binary classification problems (2 classes). It models the probability of an instance belonging to a particular class. The output is transformed using the logistic (sigmoid) function to ensure it lies between 0 and 1.\n",
    "Example: Predicting whether an email is spam (1) or not (0) is a binary classification problem suitable for logistic regression.\n",
    "\n",
    "Q2. Cost Function and Optimization in Logistic Regression:\n",
    "\n",
    "Ans:-\n",
    "\n",
    "The cost function in logistic regression is the logistic loss (also called cross-entropy loss or log loss). It measures the difference between predicted probabilities and actual class labels. The goal is to minimize this cost function.\n",
    "\n",
    "The optimization process involves using techniques like gradient descent. It iteratively updates the model's parameters to minimize the cost function. Gradient descent finds the direction in which the cost function decreases the most and adjusts the parameters accordingly.\n",
    "\n",
    "Q3. Regularization in Logistic Regression:\n",
    "\n",
    "Ans:-\n",
    "\n",
    "Regularization adds a penalty term to the cost function to discourage large parameter values. It helps prevent overfitting by reducing the model's complexity. Two common types of regularization are L1 (Lasso) and L2 (Ridge) regularization. They encourage sparse parameter values (L1) or small parameter values (L2).\n",
    "\n",
    "Q4. ROC Curve and Performance Evaluation:\n",
    "\n",
    "Ans:-\n",
    "\n",
    "The Receiver Operating Characteristic (ROC) curve is a graphical representation of the true positive rate (sensitivity) against the false positive rate (1-specificity) for various thresholds of a binary classification model. It helps visualize and evaluate the trade-off between sensitivity and specificity. The Area Under the ROC Curve (AUC-ROC) is a common metric to quantify the model's performance; a higher AUC indicates better performance.\n",
    "\n",
    "Q5. Feature Selection in Logistic Regression:\n",
    "Common techniques include:\n",
    "\n",
    "Ans:-\n",
    "\n",
    "\n",
    "Stepwise Selection: Adding or removing features based on statistical significance or model performance.\n",
    "L1 Regularization: Automatically selects relevant features while shrinking irrelevant ones to zero.\n",
    "Feature Importance: Using techniques like random forests to rank features by importance and selecting the top ones.\n",
    "These techniques help improve the model's performance by reducing noise and avoiding overfitting.\n",
    "\n",
    "Q6. Handling Imbalanced Datasets in Logistic Regression:\n",
    "\n",
    "Ans:-\n",
    "\n",
    "Imbalanced datasets have unequal class distributions. Strategies include:\n",
    "\n",
    "Resampling: Oversampling the minority class or undersampling the majority class.\n",
    "Synthetic Data: Generating synthetic samples for the minority class (e.g., SMOTE).\n",
    "Class Weights: Assigning higher weights to the minority class to balance the learning process.\n",
    "Anomaly Detection: Treating the minority class as anomalies and using anomaly detection techniques.\n",
    "\n",
    "Q7. Common Issues and Challenges:\n",
    "\n",
    "Ans:-\n",
    "\n",
    "\n",
    "Multicollinearity: When independent variables are highly correlated, it can lead to unstable coefficient estimates. Address by using techniques like PCA, Lasso regularization, or removing one of the correlated variables.\n",
    "Convergence Issues: Gradient descent might converge slowly or not at all. Adjust learning rates, use different optimization algorithms, or normalize/standardize features.\n",
    "Overfitting: Regularization can help prevent overfitting. Also, cross-validation and reducing the number of features can mitigate this issue.\n",
    "Model Interpretability: Logistic regression can be easily interpretable, but complex interactions might be missed. Polynomial features or more advanced models might be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39f7e98-1ac3-442d-a004-2e5232734c90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
